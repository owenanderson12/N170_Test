# Import necessary libraries
import os
import numpy as np
import mne
import matplotlib.pyplot as plt
import seaborn as sns

# TensorFlow and Keras
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization,
                                     Activation, MaxPooling2D, Dropout, Flatten, Dense)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report

# Suppress TensorFlow warnings (optional)
import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)

# Data Preparation
# Define the data directory where subject folders are located
data_dir = '/Users/owenanderson/Documents/NeurEx/Projects/P1_N170/Data/Data/'

# List all subject folders
subject_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]

# Initialize lists to hold data and labels from all subjects
X_list = []
y_list = []

# Loop over each subject folder
for subject in subject_folders:
    subject_data_dir = os.path.join(data_dir, subject)
    # Construct file paths for the subject's data
    X_file_ch = os.path.join(subject_data_dir, f'Epochs_{subject}.fif')
    X_file = os.path.join(subject_data_dir, f'X_eeg_{subject}.npy')
    y_file = os.path.join(subject_data_dir, f'y_labels_{subject}.npy')
    
    # Check if data files exist
    if os.path.exists(X_file) and os.path.exists(y_file):
        print(f'Loading data for {subject}')
        # Load the data
        X_subject_ch = mne.read_epochs(X_file_ch, preload=True, verbose=False)
        y_subject = np.load(y_file)
        x_subject = np.load(X_file)
        print(len(x_subject))
        print(len(y_subject))
        # Append to the list
        #X_sub_data = X_subject.get_data()
        X_list.append(x_subject)
        y_list.append(y_subject)

    else:
        print(f'Data files not found for {subject}')

# Ensure that at least one subject has been loaded
if len(X_list) == 0:
    raise ValueError("No data was loaded. Please check your data directory and files.")

# Get channel names from the last loaded subject
channels = X_subject_ch.ch_names
coi = ['P8', 'PO8', 'O2', 'P10']
chan_idx = [channels.index(chan) for chan in coi]

# Concatenate data from all subjects
X = np.concatenate(X_list, axis=0)
X = X[:, chan_idx, :]  # Select channels of interest
y = np.concatenate(y_list, axis=0)

print(f'Combined data shape before filtering: X={X.shape}, y={y.shape}')

# Filter stimulus events
stimulus_labels = [1, 2]  # 1: Face, 2: Car
stimulus_mask = np.isin(y, stimulus_labels)
X = X[stimulus_mask]
y = y[stimulus_mask]

# Adjust labels to start from 0
y = y - 1

np.save('/Users/owenanderson/Documents/NeurEx/Projects/P1_N170/Data/Workshop_data/X_data', X)
np.save('/Users/owenanderson/Documents/NeurEx/Projects/P1_N170/Data/Workshop_data/y_data', y)

# Focus on the N170 response by selecting data around 170 ms
tmin, tmax = 0.1, 0.2  # 100 ms to 200 ms
times = x_subject.times  # Time vector from the epochs
time_mask = (times >= tmin) & (times <= tmax)
X = X[:, :, time_mask]

print(f'Combined data shape after filtering: X={X.shape}, y={y.shape}')

# Normalize data per channel
def normalize_per_channel(X_data):
    X_norm = np.zeros_like(X_data)
    for i in range(X_data.shape[1]):  # Loop over channels
        scaler = StandardScaler()
        X_channel = X_data[:, i, :]
        X_norm[:, i, :] = scaler.fit_transform(X_channel)
    return X_norm

X = normalize_per_channel(X)

# Reshape data for Conv2D (samples, channels, times, 1)
X = X.transpose((0, 1, 2))  # Shape: (samples, channels, times)
X = X[..., np.newaxis]  # Shape: (samples, channels, times, 1)

# Data Augmentation
def augment_data(X, y):
    X_augmented = []
    y_augmented = []
    for i in range(X.shape[0]):
        # Original data
        X_augmented.append(X[i])
        y_augmented.append(y[i])

        # Time-shifted data
        for shift in [-2, -1, 1, 2]:  # Shift by 1 or 2 time steps
            X_shifted = np.roll(X[i], shift, axis=2)
            X_augmented.append(X_shifted)
            y_augmented.append(y[i])

        # Noise-injected data
        noise = np.random.normal(0, 0.01, X[i].shape)
        X_noisy = X[i] + noise
        X_augmented.append(X_noisy)
        y_augmented.append(y[i])

    return np.array(X_augmented), np.array(y_augmented)

# Apply augmentation on the whole dataset before splitting
X_aug, y_aug = augment_data(X, y)

print(f'Augmented data shape: X={X_aug.shape}, y={y_aug.shape}')

# Split data into training and testing sets
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_aug, y_aug, test_size=0.2, random_state=42, stratify=y_aug)

# Model Implementation using DeepConvNet
def DeepConvNet(nb_classes, Chans=4, Samples=50, dropoutRate=0.5):
    input_main = Input((Chans, Samples, 1))
    
    # Block 1
    block1 = Conv2D(25, (1, 5), padding='same', use_bias=False)(input_main)
    block1 = Conv2D(25, (Chans, 1), use_bias=False)(block1)
    block1 = BatchNormalization()(block1)
    block1 = Activation('elu')(block1)
    block1 = MaxPooling2D((1, 2))(block1)
    block1 = Dropout(0.2)(block1)
    
    # Block 2
    block2 = Conv2D(50, (1, 5), padding='same', use_bias=False)(block1)
    block2 = BatchNormalization()(block2)
    block2 = Activation('elu')(block2)
    block2 = MaxPooling2D((1, 2))(block2)
    block2 = Dropout(0.3)(block2)
    
    # Block 3
    block3 = Conv2D(100, (1, 5), padding='same', use_bias=False)(block2)
    block3 = BatchNormalization()(block3)
    block3 = Activation('elu')(block3)
    block3 = MaxPooling2D((1, 2))(block3)
    block3 = Dropout(0.4)(block3)
    
    # Block 4
    block4 = Conv2D(200, (1, 5), padding='same', use_bias=False)(block3)
    block4 = BatchNormalization()(block4)
    block4 = Activation('elu')(block4)
    block4 = MaxPooling2D((1, 2))(block4)
    block4 = Dropout(0.5)(block4)
    
    # Flatten and Dense Layers
    flatten = Flatten()(block4)
    dense = Dense(nb_classes)(flatten)
    softmax = Activation('softmax')(dense)
    
    return Model(inputs=input_main, outputs=softmax)

# Parameters for DeepConvNet
Chans = X.shape[1]
Samples = X.shape[2]
nb_classes = 2  # Binary classification

# Compile the model
model = DeepConvNet(nb_classes=nb_classes, Chans=Chans, Samples=Samples, dropoutRate=0.5)
model.compile(optimizer=Adam(learning_rate=1e-3),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Adjust EarlyStopping and ReduceLROnPlateau
early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,
                              min_lr=1e-6, verbose=1)

# Training
epochs = 500
batch_size = 16  # Keep batch size small for better gradient estimation

# Train the model
history = model.fit(X_train_val, y_train_val,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_data=(X_test, y_test),
                    callbacks=[early_stopping, reduce_lr],
                    verbose=1)

# Evaluation and Visualization
# Evaluate on test data
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')

# Visualize training history
# Plot accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Predictions
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)

# Confusion Matrix and Classification Report
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Face', 'Car'],
            yticklabels=['Face', 'Car'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, y_pred, target_names=['Face', 'Car']))
